{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!unzip archive.zip -d monkey"
      ],
      "metadata": {
        "id": "G6xnx4S4MHA8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "IVDGwLDwRw1d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "train_dir = \"/content/monkey/training/training\"\n",
        "val_dir   = \"/content/monkey/validation/validation\"\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255,rotation_range=20,width_shift_range=0.1, height_shift_range=0.1,\n",
        "    horizontal_flip=True, zoom_range=0.1)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(train_dir,target_size=(128,128),batch_size=32,class_mode='categorical')\n",
        "val_gen = val_datagen.flow_from_directory(val_dir,target_size=(128,128),batch_size=32,class_mode='categorical')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0uifuW63Kwzv",
        "outputId": "69fb2b1d-60b3-4abd-9c10-8781222802ed"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1098 images belonging to 10 classes.\n",
            "Found 272 images belonging to 10 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# CNN\n",
        "scratch_cnn = models.Sequential([\n",
        "    layers.Input(shape=(128, 128, 3)),\n",
        "    layers.Conv2D(32, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(64, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Conv2D(128, (3,3), activation='relu'),\n",
        "    layers.MaxPooling2D((2,2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(10, activation='softmax'),\n",
        "])\n",
        "\n",
        "scratch_cnn.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy']\n",
        ")\n",
        "\n",
        "#train & validate\n",
        "history_scratch = scratch_cnn.fit(train_gen,validation_data=val_gen,epochs=10)\n",
        "\n",
        "#final validation accuracy\n",
        "val_loss, val_acc = scratch_cnn.evaluate(val_gen)\n",
        "print(f\"Scratch CNN Validation accuracy: {val_acc:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0bQLcUWXPFhZ",
        "outputId": "981196d3-6dab-4ff7-e1be-0608a026cede"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.0894 - loss: 2.3703 - val_accuracy: 0.2096 - val_loss: 2.1162\n",
            "Epoch 2/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.2726 - loss: 2.0860 - val_accuracy: 0.3493 - val_loss: 1.8441\n",
            "Epoch 3/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.3453 - loss: 1.8671 - val_accuracy: 0.4853 - val_loss: 1.5210\n",
            "Epoch 4/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4156 - loss: 1.6794 - val_accuracy: 0.5074 - val_loss: 1.3853\n",
            "Epoch 5/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4036 - loss: 1.6049 - val_accuracy: 0.5588 - val_loss: 1.2834\n",
            "Epoch 6/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4739 - loss: 1.4346 - val_accuracy: 0.5772 - val_loss: 1.2253\n",
            "Epoch 7/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 1s/step - accuracy: 0.4967 - loss: 1.3591 - val_accuracy: 0.5625 - val_loss: 1.2774\n",
            "Epoch 8/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 1s/step - accuracy: 0.4941 - loss: 1.3806 - val_accuracy: 0.6140 - val_loss: 1.1723\n",
            "Epoch 9/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.5256 - loss: 1.3575 - val_accuracy: 0.5184 - val_loss: 1.2578\n",
            "Epoch 10/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.5176 - loss: 1.3507 - val_accuracy: 0.6213 - val_loss: 1.1768\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 539ms/step - accuracy: 0.5953 - loss: 1.1992\n",
            "Scratch CNN Validation accuracy: 0.621\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jmLtXK4HqdpI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras import layers, models\n",
        "import tensorflow as tf\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    zoom_range=0.1)\n",
        "\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_gen = train_datagen.flow_from_directory(\"/content/monkey/training/training\",target_size=(128, 128),batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "val_gen = val_datagen.flow_from_directory(\n",
        "    \"/content/monkey/validation/validation\",\n",
        "    target_size=(128, 128),batch_size=32,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "#load the MobileNetV2 base as feature extractor\n",
        "base_model = MobileNetV2(\n",
        "    input_shape=(128, 128, 3),\n",
        "    include_top=False,\n",
        "    weights='imagenet'\n",
        ")\n",
        "base_model.trainable = False\n",
        "\n",
        "#\n",
        "inputs = layers.Input(shape=(128, 128, 3))\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.3)(x)\n",
        "outputs = layers.Dense(10, activation='softmax')(x)\n",
        "model = models.Model(inputs, outputs)\n",
        "\n",
        "#head only\n",
        "model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history_head = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=10\n",
        ")\n",
        "\n",
        "# evaluate head-only performance\n",
        "loss_head, acc_head = model.evaluate(val_gen)\n",
        "print(f\"Head-only Validation Accuracy: {acc_head:.3f}\")\n",
        "\n",
        "#unfreezign top convolutional layers for fine-tuning\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "history_fine = model.fit(\n",
        "    train_gen,\n",
        "    validation_data=val_gen,\n",
        "    epochs=5\n",
        ")\n",
        "\n",
        "# final evaluation after fine-tuning\n",
        "loss_fine, acc_fine = model.evaluate(val_gen)\n",
        "print(f\"Fine-tuned Validation Accuracy: {acc_fine:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "EDH840ZUvYNK",
        "outputId": "984ca0a0-b7d0-48bb-e3b5-cb50505fe10a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1098 images belonging to 10 classes.\n",
            "Found 272 images belonging to 10 classes.\n",
            "Epoch 1/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.5253 - loss: 1.4756 - val_accuracy: 0.9375 - val_loss: 0.2457\n",
            "Epoch 2/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.8968 - loss: 0.3079 - val_accuracy: 0.9559 - val_loss: 0.1843\n",
            "Epoch 3/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9211 - loss: 0.2111 - val_accuracy: 0.9449 - val_loss: 0.1671\n",
            "Epoch 4/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9483 - loss: 0.1582 - val_accuracy: 0.9375 - val_loss: 0.2056\n",
            "Epoch 5/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9677 - loss: 0.0996 - val_accuracy: 0.9522 - val_loss: 0.1579\n",
            "Epoch 6/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9704 - loss: 0.0813 - val_accuracy: 0.9485 - val_loss: 0.1907\n",
            "Epoch 7/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9741 - loss: 0.0696 - val_accuracy: 0.9375 - val_loss: 0.2181\n",
            "Epoch 8/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9817 - loss: 0.0617 - val_accuracy: 0.9522 - val_loss: 0.1943\n",
            "Epoch 9/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 1s/step - accuracy: 0.9734 - loss: 0.0877 - val_accuracy: 0.9522 - val_loss: 0.1781\n",
            "Epoch 10/10\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 1s/step - accuracy: 0.9845 - loss: 0.0562 - val_accuracy: 0.9559 - val_loss: 0.1899\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 878ms/step - accuracy: 0.9525 - loss: 0.1729\n",
            "Head-only Validation Accuracy: 0.956\n",
            "Epoch 1/5\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 1s/step - accuracy: 0.8935 - loss: 0.3478 - val_accuracy: 0.9559 - val_loss: 0.1828\n",
            "Epoch 2/5\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 1s/step - accuracy: 0.9046 - loss: 0.3242 - val_accuracy: 0.9559 - val_loss: 0.1839\n",
            "Epoch 3/5\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.9185 - loss: 0.2634 - val_accuracy: 0.9559 - val_loss: 0.1860\n",
            "Epoch 4/5\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 1s/step - accuracy: 0.9107 - loss: 0.2086 - val_accuracy: 0.9485 - val_loss: 0.1897\n",
            "Epoch 5/5\n",
            "\u001b[1m35/35\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 1s/step - accuracy: 0.9469 - loss: 0.1576 - val_accuracy: 0.9485 - val_loss: 0.1894\n",
            "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 889ms/step - accuracy: 0.9404 - loss: 0.2313\n",
            "Fine-tuned Validation Accuracy: 0.949\n"
          ]
        }
      ]
    }
  ]
}